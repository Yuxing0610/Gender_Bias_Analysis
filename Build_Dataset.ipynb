{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA_fe4oxWT6p"
      },
      "source": [
        "# Mouting the Google Drive\n",
        "\n",
        "First, mount the google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEcIlRwfWY4C",
        "outputId": "ca009ebf-1edb-4ce2-841c-b87dfd38d19a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHsYlfK7e5a"
      },
      "source": [
        "# Build dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwv0XY95IOoz"
      },
      "source": [
        "We intend to extract datasets of various mainstream media like New York Times and Fox News of different categories from the whole dataset provided. The following code is to generate the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAPhTqRtdHQh"
      },
      "source": [
        "# We try to split the dataset of one media into seven topics.\n",
        "# The map maps the categories to the keywords. \n",
        "# We try to reuse the categorizing result of the original website and investigate the patterns of the url of different media websites. \n",
        "# If the url of the quotation contains those keywords, we assume that the quotation belongs to the categories.\n",
        "\n",
        "categories = {\n",
        "'politics': ['/politics/', '/us/', '/world/', '/business/', '/opinion/', '/economy/', '/finance/', '/market/'],\n",
        "'technology': ['/tech/', '/science/'],\n",
        "'health': ['/health/'],\n",
        "'sports': ['/sports/'],\n",
        "'arts': ['/art/', '/arts/', '/movie/', '/fashion/', '/book/', '/books/' '/style/', '/music/', '/entertainment/'],\n",
        "'lifestyle': ['/food/', '/travel/', '/lifestyle/', '/auto/'],\n",
        "'uncategorized': ['/']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zskZSNwdeWZS",
        "outputId": "a46b5bce-ce1c-4ba0-d6e3-9873c94141a4"
      },
      "source": [
        "# Import the packages we need.\n",
        "\n",
        "import bz2\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare the necessary path data and file handlers.\n",
        "\n",
        "years = ['2015', '2016', '2017', '2018', '2019', '2020']\n",
        "path_to_file_list = ['/content/drive/MyDrive/Quotebank/quotes-' + year + '.json.bz2' for year in years]\n",
        "path_to_out_list = ['/content/drive/Shareddrives/ADA/foxnews/'+year+'/quotes-' + year + '-fox-all.json.bz2' for year in years]\n",
        "gender_file_list = ['/content/drive/MyDrive/quotes-' + year + '-fox-gender.json.bz2' for year in years]\n",
        "\n",
        "maps = []\n",
        "for year in years:\n",
        "  cat_to_file = {}\n",
        "  for cat in categories:\n",
        "    path = '/content/drive/MyDrive/quotes-' + year + '-fox-' + cat + '.json.bz2'\n",
        "    cat_to_file[cat] = bz2.open(path, 'wb')\n",
        "  maps.append(cat_to_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjCaa8GNSKrL"
      },
      "source": [
        "# Select the quotations according the prementioned rules and write them to new files.\n",
        "\n",
        "for i, year in enumerate(years):\n",
        "  path_to_file = path_to_file_list[i]\n",
        "  path_to_out = path_to_out_list[i]\n",
        "  with bz2.open(path_to_file, 'rb') as s_file:\n",
        "    with bz2.open(path_to_out, 'wb') as d_file:\n",
        "      for instance in s_file: \n",
        "        instance = json.loads(instance) # loading a sample\n",
        "        urls = instance['urls'] # extracting list of links\n",
        "        is_media = False\n",
        "        category = None\n",
        "        for url in urls:\n",
        "          o = urlparse(url)\n",
        "          if 'foxnews' in o.netloc:\n",
        "            for cat in categories:\n",
        "              for keyword in categories[cat]:\n",
        "                if keyword in o.path:\n",
        "                  category = cat\n",
        "                  is_media = True\n",
        "                  break\n",
        "              if is_media:\n",
        "                break\n",
        "            if is_media:\n",
        "              break\n",
        "        if not is_media:\n",
        "          continue\n",
        "    \n",
        "        instance['category'] = category\n",
        "        d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) \n",
        "\n",
        "  with bz2.open(path_to_out, 'rb') as s_file:\n",
        "    for instance in s_file:\n",
        "      instance = json.loads(instance)\n",
        "      cat = instance['category']\n",
        "      maps[i][cat].write((json.dumps(instance)+'\\n').encode('utf-8')) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avb2Rr0xVx5e"
      },
      "source": [
        "We also want to extract the gender related quotations from the dataset. We prepared a gender related word list. If a quotation or its url contains some words in the list, we assume it to be gender related."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WccaZaGE6prk"
      },
      "source": [
        "# Load the gender word list.\n",
        "\n",
        "with open('/content/drive/Shareddrives/ADA/gender related words.txt', 'r') as infile:\n",
        "  words = infile.readlines()\n",
        "\n",
        "gender_word_set = set([w.strip() for w in words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8NQwb4IixAB"
      },
      "source": [
        "# Preprocess the text.\n",
        "\n",
        "def clean_text(text):\n",
        "    # To remove the punctuations\n",
        "    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n",
        "    # To consider only alphabets and numerics\n",
        "    text = re.sub('[^a-zA-Z]',' ',text) \n",
        "    # To replace newline with space\n",
        "    text = re.sub(\"\\n\",\" \",text)\n",
        "    # To convert to lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    return text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-30Oa4TpWOfh"
      },
      "source": [
        "# Select the quotations according the prementioned rules and write them to new files.\n",
        "\n",
        "for i, year in enumerate(years):\n",
        "  path_to_file = path_to_out_list[i]\n",
        "  path_to_out = gender_file_list[i]\n",
        "  with bz2.open(path_to_file, 'rb') as s_file:\n",
        "    with bz2.open(path_to_out, 'wb') as d_file:\n",
        "      for instance in s_file: \n",
        "        instance = json.loads(instance)\n",
        "        urls = instance['urls']\n",
        "        quotation = instance['quotation']\n",
        "\n",
        "        is_gender_related = False\n",
        "        \n",
        "        for url in urls:\n",
        "          o = urlparse(url)\n",
        "          path = clean_text(o.path)\n",
        "          for w in path:\n",
        "            if w in gender_word_set:\n",
        "              is_gender_related = True\n",
        "\n",
        "          if is_gender_related:\n",
        "            break\n",
        "\n",
        "        if not is_gender_related:\n",
        "          quotation = clean_text(quotation)\n",
        "          for w in quotation:\n",
        "            if w in gender_word_set:\n",
        "              is_gender_related = True\n",
        "              break\n",
        "\n",
        "        if is_gender_related:\n",
        "          d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}